# LLM project - RAG vs Fine-tuning

### Victor TAN & Noah KWA MOUTOME - Group 4


## Notes:

Need around 16 GB CUDA memory for current implementations,

Need around 1h30min for complete execution,

You can change the batch size and model size if necessary.



## Files:

[1FINE_TUNING.ipynb](1FINE_TUNING.ipynb): Fine-tuning notebook, from base-model to QLoRA fine-tuned on dataset,

[2EVAL.ipynb](2EVAL.ipynb): Evaluation notebook, compute metrics for comparison (either base-model or fine-tuned model),

[3RAG.ipynb](3RAG.ipynb): RAG evaluation notebook, compute metrics using RAG for comparison (either base-model or fine-tuned model),

[qa_lol.json](qa_lol.json): Dataset for specialized subject,

[article.pdf](article.pdf): Original article reviewed,

[Slides.pdf](Slides.pdf): Slides for presentation.
